{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Transfer Learning\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Activity 2c: Exploring Transfer Learning with CIFAR-10\n",
    "<br>\n",
    "\n",
    "- Objective:\n",
    "\n",
    "    In this activity, you'll study the concept of Transfer Learning, a powerful technique to improve the performance of your models by leveraging pre-trained architectures. The provided notebook offers a complete solution using a specific pre-trained model on the CIFAR-10 dataset. Your task is to extend this by trying out two other pre-trained models.\n",
    "    \n",
    "- Instructions:\n",
    "\n",
    "    This activity should be submitted in the same format as previous activities. Remember to include the names of all team members in a markdown cell at the beginning of the notebook. The grade obtained in this notebook will be averaged with that of Activity 2b, for the grade of Activity 2.    \n",
    "\n",
    "    Study the Provided Code: The provided notebook has a complete Transfer Learning solution using a particular pre-trained model. Make sure you understand the flow of the code and the role of each component.\n",
    "\n",
    "    Select Two Other Pre-trained Models: Choose two different pre-trained models available in PyTorch's model zoo.\n",
    "\n",
    "    Apply Transfer Learning: Add cells to implement Transfer Learning using the two models you've chosen. Train these models on the CIFAR-10 dataset.\n",
    "\n",
    "    Evaluation: After training, evaluate your models' performance. Compare the results with the provided solution and try to interpret why there might be differences.\n",
    "\n",
    "    Documentation: In a markdown cell, summarize your findings. Include any challenges you faced, how you overcame them, and any interesting insights you gained from comparing the different pre-trained models.\n",
    "\n",
    "- Note:\n",
    "\n",
    "    Although the provided code serves as a guide, you're encouraged to implement the new solutions on your own. The goal is to reinforce your understanding of Transfer Learning and how to apply it effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lead Professor\n",
    "###Dr. Jose Antonio Cantoral-Ceballos\n",
    "###Asistant Professor\n",
    "###Dr. Carlos Villaseñor\n",
    "#Team 40\n",
    "#Members:\n",
    "####Iossif Moises Palli Laura A01794457\n",
    "####Astrid Rosario Bernaga Torres A01793080\n",
    "####Cecilia Acevedo Rodríguez A01793953\n",
    "####Fredy Reyes Sánchez A01687370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = '/media/pepe/DataUbuntu/Databases/cifar-10/cifar-10-batches-py'\n",
    "DATA_PATH = '/home/pepe/Documents/github_repos/datasets/cifar-10-batches-py'\n",
    "\n",
    "# Number of training examples to be used\n",
    "NUM_TRAIN = 45000\n",
    "# Batch size for training\n",
    "MINIBATCH_SIZE = 64\n",
    "# Transformation of images using ImageNet values (scaling and normalization)\n",
    "transform_imagenet = T.Compose([\n",
    "                T.Resize(224),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "            ])\n",
    "\n",
    "# Image transformation for CIFAR-10 (only normalization)\n",
    "transform_cifar = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.491, 0.482, 0.447], [0.247, 0.243, 0.261])\n",
    "            ])\n",
    "\n",
    "# Training set loader\n",
    "cifar10_train = datasets.CIFAR10(DATA_PATH, train=True, download=True,\n",
    "                             transform=transform_imagenet)\n",
    "train_loader = DataLoader(cifar10_train, batch_size=MINIBATCH_SIZE,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "# Validation set loader\n",
    "cifar10_val = datasets.CIFAR10(DATA_PATH, train=True, download=True,\n",
    "                           transform=transform_imagenet)\n",
    "val_loader = DataLoader(cifar10_val, batch_size=MINIBATCH_SIZE,\n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, len(cifar10_val))))\n",
    "\n",
    "# Testing set loader\n",
    "cifar10_test = datasets.CIFAR10(DATA_PATH, train=False, download=True,\n",
    "                            transform=transform_imagenet)\n",
    "test_loader = DataLoader(cifar10_test, batch_size=MINIBATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using  GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a CUDA-compatible GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of class labels for CIFAR-10 dataset\n",
    "classes = ['Plane', 'Car', 'Bird', 'Cat', 'Deer','Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "# Function to plot and display an image\n",
    "def plot_figure(image):\n",
    "    plt.imshow(image.permute(1,2,0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Randomly sample an image index from the test data\n",
    "rnd_sample_idx = np.random.randint(len(test_loader))\n",
    "print(f'La imagen muestreada representa un: {classes[test_loader.dataset[rnd_sample_idx][1]]}')\n",
    "image = test_loader.dataset[rnd_sample_idx][0]\n",
    "image = (image - image.min()) / (image.max() -image.min() )\n",
    "plot_figure(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def accuracy(model, loader):\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    model.eval()\n",
    "    model = model.to(device=device)\n",
    "    with torch.no_grad():\n",
    "        for (xi, yi) in loader:\n",
    "            xi = xi.to(device=device, dtype = torch.float32)\n",
    "            yi = yi.to(device=device, dtype = torch.long)\n",
    "            scores = model(xi) # mb_size, 10\n",
    "            _, pred = scores.max(dim=1) #pred shape (mb_size )\n",
    "            num_correct += (pred == yi).sum() # pred shape (mb_size), yi shape (mb_size, 1)\n",
    "            num_total += pred.size(0)\n",
    "        return float(num_correct)/num_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preloaded charging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(model_resnet18.parameters()):\n",
    "    print(i, w.shape, w.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aux = nn.Sequential(*list(model_resnet18.children()))\n",
    "model_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aux = nn.Sequential(*list(model_resnet18.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, parameter in enumerate(model_aux.parameters()):\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, parameter in enumerate(model_aux.parameters()):\n",
    "    print(i, parameter.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, epochs=100):\n",
    "#     def train(model, optimiser, scheduler = None, epochs=100):\n",
    "    model = model.to(device=device)\n",
    "    for epoch in range(epochs):\n",
    "        for i, (xi, yi) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            xi = xi.to(device=device, dtype=torch.float32)\n",
    "            yi = yi.to(device=device, dtype=torch.long)\n",
    "            scores = model(xi)\n",
    "\n",
    "            cost = F.cross_entropy(input= scores, target=yi)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            cost.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        acc = accuracy(model, val_loader)\n",
    "#         if epoch%5 == 0:\n",
    "        print(f'Epoch: {epoch}, costo: {cost.item()}, accuracy: {acc},')\n",
    "#         scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = 256\n",
    "hidden = 256\n",
    "lr = 5e-4\n",
    "epochs = 3\n",
    "# model1 = nn.Sequential(nn.Flatten(),\n",
    "#                        nn.Linear(in_features=32*32*3, out_features=hidden1), nn.ReLU(),\n",
    "#                        nn.Linear(in_features=hidden1, out_features=hidden), nn.ReLU(),\n",
    "#                        nn.Linear(in_features=hidden, out_features=10))\n",
    "\n",
    "model1 = nn.Sequential(model_aux,\n",
    "                       nn.Flatten(),\n",
    "                       nn.Linear(in_features=512, out_features= 10, bias= True))\n",
    "optimiser = torch.optim.Adam(model1.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# train(model1, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model1, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model VGG16\n",
    "#Select other pretrained models from PyTorch's model zoo.\n",
    "#I will use VGG16 and SQUEEZENET pretrained models\n",
    "model_vgg16 = models.vgg16(pretrained=True)\n",
    "model_squeezenet = models.squeezenet1_1(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model VGC16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply transfer learning for VGG16\n",
    "# Modify the output layer for VGG-16\n",
    "model_vgg16.classifier[6] = nn.Linear(4096, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-Tuning: Fine-tuning involves updating the weights of the pre-trained layers while keeping the initial layers (feature extraction layers) frozen.\n",
    "#This allows the model to adapt to the specific dataset.\n",
    "#We are able to achieve this by setting the requires_grad attribute of specific layers to True.\n",
    "\n",
    "for param in model_vgg16.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_vgg16, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model_vgg16, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MODEL SQUEEZENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply transfer learning for Squeezenet\n",
    "# Modify the output layer for Squeezenet\n",
    "model_squeezenet.classifier[1] = nn.Conv2d(512, 10, kernel_size=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-Tuning: Fine-tuning involves updating the weights of the pre-trained layers while keeping the initial layers (feature extraction layers) frozen.\n",
    "#This allows the model to adapt to the specific dataset.\n",
    "#We are able to achieve this by setting the requires_grad attribute of specific layers to True.\n",
    "\n",
    "for param in model_squeezenet.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_squeezenet, optimiser, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model_squeezenet, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Comparative and comments of this models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: ResNet-18**\n",
    "- Architecture: ResNet-18 is a relatively shallow model compared to VGG16 and SqueezeNet. It has 18 layers.\n",
    "- Number of Parameters: ResNet-18 has a moderate number of parameters.\n",
    "- Pre-trained Dataset: Trained on the ImageNet dataset, which is a large and diverse dataset.\n",
    "- Performance on CIFAR-10: Achieved an accuracy of around 79.7% on the CIFAR-10 dataset after three epochs.\n",
    "- Training Time: The training time for ResNet-18 is relatively fast, thanks to its smaller size.\n",
    "- Overfitting: Shows no signs of overfitting as accuracy on the validation set remains consistent.\n",
    "- Fine-Tuning: This model is relatively easy to fine-tune for your specific task.\n",
    "\n",
    "**Model: VGG16**\n",
    "- Architecture: VGG16 is a deep model with 16 weight layers (13 convolutional layers and 3 fully connected layers).\n",
    "- Number of Parameters: VGG16 has a large number of parameters, which may increase the risk of overfitting.\n",
    "- Pre-trained Dataset: Also trained on ImageNet, offering a good feature extraction base.\n",
    "- Performance on CIFAR-10: Achieved an accuracy of around 7.9% on the CIFAR-10 dataset, which is significantly lower than the other models.\n",
    "- Training Time: Training a large model like VGG16 requires more time due to its depth.\n",
    "- Overfitting: VGG16 seems to overfit the training data, as the accuracy on the validation set is much lower.\n",
    "- Fine-Tuning: Fine-tuning a model with a large number of parameters may require careful regularization.\n",
    "\n",
    "**Model: SqueezeNet**\n",
    "- Architecture: SqueezeNet is a lightweight model with a small number of parameters.\n",
    "- Number of Parameters: SqueezeNet has a very low number of parameters compared to the other models.\n",
    "- Pre-trained Dataset: Also pre-trained on ImageNet, providing good feature extraction capabilities.\n",
    "- Performance on CIFAR-10: Achieved an accuracy of around 10.07% on the CIFAR-10 dataset.\n",
    "- Training Time: Training SqueezeNet is fast due to its small size.\n",
    "- Overfitting: The model might not be prone to severe overfitting, given the consistent validation accuracy.\n",
    "- Fine-Tuning: SqueezeNet is relatively easy to fine-tune, given its smaller size.\n",
    "\n",
    "**Conclusion:**\n",
    "- ResNet-18 offers a good balance of accuracy and training time. It's a suitable choice for various tasks and provides robust performance.\n",
    "- VGG16, while powerful, may not be the best choice for the CIFAR-10 dataset due to its tendency to overfit. It may require more complex regularization techniques.\n",
    "- SqueezeNet is lightweight and fast to train, making it a practical choice for scenarios with limited computational resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
